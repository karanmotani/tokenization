Tokenization

	Problem 1: Tokenization
  
* TreeMap is used to store the tokens and the stems generated. Initially at the start of the program, the time is noted. At the end of problem 1 i.e. after tokenization, the current time is noted and the difference is calculated resulting in total time taken by the program to acquire the text characteristics.

* The program reads all the files in the directory using listFiles. The number of files are counted and passed to the tokenizer method. The method reads the file line by line. The lines that contain the SGML () tags are ignored. StringTokenizer is used to break the sentence into tokens. The number of token are counted. Every token generated by the StringTokenizer is then converted to lower case, the hyphens("-") are removed, and the non-alphanumeric characters are removed. These token are stored in the "token" TreeMap. If the particular token already exists, then its count is increased by 1, else it is stored on the TreeMap with count 1.

* For the most frequent token, the freqSort method sorts the TreeMap token into decreasing order of the count thus giving the 30 most frequent words.

* The size of the TreeMap will give the number of distinct words as TreeMap stores only distinct words.

* For words that occured only once, an iterator is set which will check for the words that have occured just once.

	Problem 2: Stemming

* Stemmer.java is used for Stemming. Each token in the token TreeMap is converted to character array and passed to method add() in Stemmer.java

* The stems are stored in the "stem" TreeMap. If the particular stem already exists, then its count is increased by 1, else it is stored on the TreeMap with count 1. The rest of the functions are similarly calculated as for Problem 1.

--------------------------------------------------------------------------

1. How long the program took to acquire the text characteristics.
> 1651.0ms

2. How the program handles:
 A. Upper and lower case words (e.g. "People", "people", "Apple", "apple");
> Every word is converted to lower case 
  Eg. People becomes people

 B. Words with dashes (e.g. "1996-97", "middle-class", "30-year", "tean-ager")
>  The hyphens are removed and replaced by space.
   Eg. middle-class becomes midde class
 
 C. Possessives (e.g. "sheriff's", "university's")
> All the non-alphanumeric characters are removed
  Eg. sheriff's becomes sheriffs

 D. Acronyms (e.g., "U.S.", "U.N.")
> The period are removed and all the characters are already converted to lower case.
  Eg. U.S. becomes us 

3. Briefly discuss your major algorithms and data structures
>
How the program runs?
* The program Tokenization.java initially accepts the location of the Cranfield text collection as a command line argument

--------------------------------------------------------------------------

OUTPUT of the program:

	Problem 1: Tokenization

1. The number of tokens in the Cranfield text collection: 235890

2. The number of unique words in the Cranfield text collection: 11502

3. The number of words that occur only once in the Cranfield text collection: 5468

4. The 30 most frequent words in the Cranfield text collection and respective frequency information: 

#	Token	  Frequency
1	the	    19448
2	of	    12714
3	and	    6669
4	a	    5922
5	in	    4644
6	to	    4560
7	is	    4113
8	for	    3491
9	are	    2428
10	with	    2263
11	on	    1943
12	flow	    1848
13	at	    1834
14	by	    1755
15	that	    1570
16	an	    1388
17	be	    1271
18	pressure	    1207
19	boundary	    1156
20	from	    1116
21	as	    1114
22	this	    1081
23	layer	    1002
24	which	    975
25	number	    973
26	results	    885
27	it	    854
28	mach	    823
29	theory	    788
30	shock	    712

5. The average number of word tokens per document: 168

6. Time taken to acquire the text characteristics: 1651.0ms

----------------------------------------------------------------------------------------------------
	Problem 2: Stemming

1. The number of distinct stems in the Cranfield text collection: 8718

2. The number of stems that occur only once in the Cranfield text collection: 4336

3. The 30 most frequent stems in the Cranfield text collection and respective frequency information: 

#	Stem	  Frequency
1	the	    19448
2	of	    12714
3	and	    6669
4	a	    5922
5	in	    4644
6	to	    4560
7	is	    4113
8	for	    3491
9	ar	    2456
10	with	    2263
11	on	    2261
12	flow	    2079
13	at	    1834
14	by	    1755
15	that	    1570
16	an	    1388
17	pressur	    1382
18	be	    1368
19	number	    1347
20	boundari	    1185
21	layer	    1134
22	from	    1116
23	as	    1114
24	result	    1087
25	thi	    1081
26	it	    1041
27	effect	    996
28	which	    975
29	method	    886
30	theori	    881

4. The average number of word stems per document: 168
